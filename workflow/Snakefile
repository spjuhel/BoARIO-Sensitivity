# Main entrypoint of the workflow.
# Please follow the best practices:
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there.
conda:
    "envs/global.yaml"

from snakemake.utils import min_version
from snakemake.utils import Paramspace
import others.regex_patterns as regex_patterns
import pandas as pd

min_version("8.0.0")


#### Config file ####
configfile: "config/config.yaml"


#### report [WIP] ####
# report : "report/workflow.rst"


module mriot:
    snakefile:
        # here, it is also possible to provide a plain raw URL like "https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling/raw/v2.0.1/workflow/Snakefile"
        "/cluster/home/sjuhel/repos/BoARIO-MRIOT-Tools/workflow/Snakefile"
    config:
        config["data-mriot"]
    prefix:
        config["data-mriot"]["prefix"]


use rule * from mriot exclude all_mriot


#### wildcard constraints ####
wildcard_constraints:
    year=r"\d\d\d\d",
    aggregation=r"full_exio3compat|74_sectors",
    scope=r"general|local",
    focus=r"all_sim|no_absurd",


ALL_MRIOT = expand(
    "{mriot_data}{parsed}/{mrio_type}/{mrio_type}_{year}_full.pkl",
    mriot_data=config["data-mriot"]["prefix"],
    parsed=config["data-mriot"]["parsed_mriot_dir"],
    mrio_type=["exiobase3_ixi", "euregio", "icio2021"],
    year=[2000, 2010],
)

localrules:
    make_rst_report,


rule all_mriot:
    input:
        ALL_MRIOT,

if config["testing"]:
    paramspace = Paramspace(
        pd.read_csv(config["parameters space test"]),
        filename_params=[
            "order",
            "psi",
            "base_alpha",
            "max_alpha",
            "tau_alpha",
        ],
    )
else:
    paramspace = Paramspace(
        pd.read_csv(config["parameters space"]),
        filename_params=[
            "order",
            "psi",
            "base_alpha",
            "max_alpha",
            "tau_alpha",
        ],
    )


rule make_rst_report:
    input:
        expand(
            "report/source/results-pages/{focus}-results.rst",
            focus=config["focus"].keys(),
        ),
    output:
        touch("report/report_pushed"),
    conda:
        "envs/boario-sensi-report.yml"
    shell:
        """
        git add report/source;
        git add report/images;
        git commit -m "update results from workflow";
        git push;
        """


rule make_all_focus_results:
    input:
        local_rst=expand(
            "report/source/results-pages/results-contents/local-{{focus}}-{selection_type}-{faceting}-results.rst",
            selection_type=["recovery_sce"],
            faceting=["Experience~mrio"],
        ),
        general_rst=expand(
            "report/source/results-pages/results-contents/general-{{focus}}-{selection_type}-{faceting}-results.rst",
            selection_type=["cumsum_impact_class"],
            faceting=["sectorXregion~Experience"],
        ),
    params:
        variables=config["variables"],
    output:
        "report/source/results-pages/{focus}-results.rst",
    log:
        "logs/make_rst_{focus}-results.log",
    conda:
        "envs/boario-sensi-report.yml"
    script:
        "scripts/generate_results_index_rst.py"


def get_grids(wildcards):
    grids = []
    df_path = checkpoints.prep_plot_df.get(**wildcards).output[0]
    plot_df = pd.read_parquet(df_path)
    if config["focus"][wildcards.focus] != "all":
        plot_df.query(config["focus"][wildcards.focus], inplace=True)
    selections = plot_df[wildcards.selection_type].sort_values().unique()
    grids += expand(
        "{dirs}/{scope}/{focus}/{selection_type}~{selection}/{faceting}/{variable}_{plot_type}.{ext}",
        dirs=["results/figs", "report/images/figs"],
        scope=wildcards.scope,
        selection_type=wildcards.selection_type,
        selection=selections,
        focus=wildcards.focus,
        faceting=wildcards.faceting,
        variable=config["variables"],
        plot_type=config["plot config"]["plot types"],
        ext="svg",
    )
    return grids


rule make_rst_result:
    input:
        plot_df="results/{scope}-plot_df.parquet",
        grids=get_grids,
    output:
        res_rst="report/source/results-pages/results-contents/{scope}-{focus}-{selection_type}-{faceting}-results.rst",
    params:
        variables=config["variables"],
    log:
        "logs/make_rst_results-{scope}-{focus}-{selection_type}-{faceting}.log",
    conda:
        "envs/boario-sensi-report.yml"
    script:
        "scripts/generate_results_rst.py"


rule plot_grid:
    input:
        plot_df="results/{scope}-plot_df.parquet",
    output:
        expand(
            "{dirs}/{{scope}}/{{focus}}/{{selection_type}}~{{selection}}/{{faceting}}/{{variable}}_{{plot_type}}.{{ext}}",
            dirs=["results/figs", "report/images/figs"],
        ),
    params:
        sharey=config["plot config"]["grid"]["sharey"],
        row_order=config["plot config"]["grid"]["row order"],
    log:
        "logs/plot_results-{scope}-{focus}-{selection_type}-{selection}-{faceting}-{variable}-{plot_type}_{ext}.log",
    threads: 4
    resources:
        mem_mb_per_cpu=4000,
    benchmark:
        "benchmarks/plot_results-{scope}-{focus}-{selection_type}-{selection}-{faceting}-{variable}-{plot_type}_{ext}.log"
    conda:
        "envs/BoARIO-sensi.yml"
    script:
        "scripts/plot_results.py"


checkpoint prep_plot_df:
    input:
        "results/common_aggreg.parquet",
    output:
        "results/{scope}-plot_df.parquet",
    log:
        "logs/prep_{scope}-plot_df.log",
    threads: 4
    resources:
        mem_mb_per_cpu=3000,
    benchmark:
        "benchmarks/prep_{scope}-plot_df.log"
    conda:
        "envs/BoARIO-sensi.yml"
    script:
        "scripts/prep_plot_df.py"


def regroup_input(wildcards):
    dico = {
        var: expand(
            "results/simulations/{params}/parquets/" + var + ".parquet",
            params=paramspace.instance_patterns,
        )
        for var in config["variables"]
    }
    return dico


rule regroup_parquet_to_common_aggreg:
    input:
        unpack(regroup_input),
        sectors_aggreg_ods=config["data-mriot"]["prefix"] + config["data-mriot"]["aggregation_csv_dir"] + "/" + config["sectors_common_aggreg"],
        region_aggreg_ods=config["data-mriot"]["prefix"] + config["data-mriot"]["aggregation_csv_dir"] + "/" + config["regions_common_aggreg"],
    output:
        "results/common_aggreg.parquet",
    log:
        f"logs/regroup_aggreg.log",
    threads: 4
    benchmark:
        f"benchmarks/regroup_aggreg.log"
    conda:
        "envs/BoARIO-sensi.yml"
    script:
        "scripts/regroup_aggreg.py"


rule all_parquets:
    input:
        parquet_files=expand(
            "results/simulations/{params}/parquets/{var}.parquet",
            params=paramspace.instance_patterns,
            var=config["variables"],
        ),


def get_simulation_inputs(wildcards):
    mrio_name = wildcards.mrio
    sectors_scenario = wildcards.sectors_scenario
    filepath = (
        config["data-mriot"]["prefix"] + config["data-mriot"]["parsed_mriot_dir"]
    )  # the path to the pickle files
    regex = regex_patterns.MRIOT_FULLNAME_REGEX
    match = regex.match(mrio_name)  # match the filename with the regular expression
    if not match:
        raise ValueError(f"The file name {mrio_name} is not valid.")
    mriot_prefix, year, aggreg = (
        match.groups()
    )  # get the prefix and year from the matched groups
    fullpath = (
        filepath + "/" + mriot_prefix + "/" + mrio_name + ".pkl"
    )  # create the full file path

    parsed=config["data-mriot"]["parsed_mriot_dir"],
    sectors_config_path = (
        config["data-mriot"]["prefix"] + config["data-mriot"]["mriot_params_dir"] +
        "/" + mriot_prefix + "_" +
        config["sectors_scenarios"][sectors_scenario] +
        ".csv")
    return {"mrio": fullpath, "sectors_config": sectors_config_path}


rule simulate:
    input:
        unpack(get_simulation_inputs),
    output:
        # format a wildcard pattern like "alpha~{alpha}/beta~{beta}/gamma~{gamma}"
        # into a file path, with alpha, beta, gamma being the columns of the data frame
        output_dir=directory(f"results/simulations/{paramspace.wildcard_pattern}"),
        parquet_files=[
            f"results/simulations/{paramspace.wildcard_pattern}/parquets/"
            + var
            + ".parquet"
            for var in config["variables"]
        ],
        json_files=[
            f"results/simulations/{paramspace.wildcard_pattern}/jsons/"
            + json
            + ".json"
            for json in [
                "indexes",
                "equilibrium_checks",
                "simulated_events",
                "simulated_params",
            ]
        ],
        #.format(paramspace.wildcard_pattern,"{files}"), files=["indexes","equilibrium_checks","simulated_events","simulated_params"]),
        done=touch(f"results/simulations/{paramspace.wildcard_pattern}/sim.done"),
    params:
        # automatically translate the wildcard values into an instance of the param space
        # in the form of a dict (here: {"alpha": ..., "beta": ..., "gamma": ...})
        simulation_params=paramspace.instance,
        sim_length=config["sim_length"],
    log:
        f"logs/simulations/{paramspace.wildcard_pattern}.log",
    threads: 2
    benchmark:
        f"benchmarks/simulations/{paramspace.wildcard_pattern}.log"
    resources:
        mem_mb_per_cpu=10000,
        runtime=240,
    conda:
        "envs/BoARIO-sensi.yml"
    script:
        "scripts/simulate.py"
